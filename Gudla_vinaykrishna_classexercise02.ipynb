{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinaykrishnaGudla/vinaykrishna_Gudla_031/blob/main/Gudla_vinaykrishna_classexercise02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: How does flexible work impact remote employees' productivity and satisfaction?\n",
        "\n",
        "Data Collection:\n",
        "\n",
        "1. Survey Design:\n",
        "   - Create a survey with questions about work flexibility, productivity, and job satisfaction.\n",
        "   - Include demographic questions for diversity insights.\n",
        "\n",
        "2. Sampling:\n",
        "   - Select 500 random employees from various industries for a diverse sample.\n",
        "\n",
        "3. Consent and Anonymity:\n",
        "   - Get participant consent and assure anonymity for honest responses.\n",
        "\n",
        "4. Data Collection Method:\n",
        "   - Use online survey platforms (e.g., Google Forms).\n",
        "   - Share the survey link via email, workplace channels, and social media.\n",
        "\n",
        "5. Time Frame:\n",
        "   - Run the survey for four weeks for ample response time.\n",
        "\n",
        "6. Quantitative Data:\n",
        "   - Use scales to measure productivity and job satisfaction.\n",
        "\n",
        "7. Qualitative Data:\n",
        "   - Include open-ended questions for personal insights.\n",
        "\n",
        "8. Data Storage:\n",
        "   - Securely store data on a server, complying with privacy regulations.\n",
        "\n",
        "9. Data Analysis:\n",
        "   - Use tools like SPSS for quantitative data and thematic analysis for qualitative insights.\n",
        "\n",
        "10. Reporting:\n",
        "   - Compile a report with statistical results and key themes from qualitative responses.\n"
      ],
      "metadata": {
        "id": "0pim54kSaviN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "# Generate hypothetical data for the survey\n",
        "def generate_data():\n",
        "    data = []\n",
        "    for _ in range(1000):\n",
        "        age = random.randint(20, 60)\n",
        "        industry = random.choice(['IT', 'Finance', 'Healthcare', 'Education'])\n",
        "        flexibility_rating = random.randint(1, 5)\n",
        "        productivity_rating = random.randint(1, 5)\n",
        "        satisfaction_rating = random.randint(1, 5)\n",
        "\n",
        "        data.append({\n",
        "            'Age': age,\n",
        "            'Industry': industry,\n",
        "            'FlexibilityRating': flexibility_rating,\n",
        "            'ProductivityRating': productivity_rating,\n",
        "            'SatisfactionRating': satisfaction_rating\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Save the generated data to a CSV file\n",
        "def save_to_csv(data):\n",
        "    fields = ['Age', 'Industry', 'FlexibilityRating', 'ProductivityRating', 'SatisfactionRating']\n",
        "\n",
        "    with open('flexibility_survey_data.csv', mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fields)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    survey_data = generate_data()\n",
        "    save_to_csv(survey_data)\n",
        "    print(\"Dataset created and saved successfully.\")\n",
        "with open('flexibility_survey_data.csv', mode='r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for idx, row in enumerate(reader, start=1):\n",
        "            print(row)\n",
        "            if idx == 5:\n",
        "                break\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce2a6b3-dd4a-4e7c-deae-00cdef7eeb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created and saved successfully.\n",
            "{'Age': '48', 'Industry': 'Education', 'FlexibilityRating': '1', 'ProductivityRating': '3', 'SatisfactionRating': '4'}\n",
            "{'Age': '28', 'Industry': 'IT', 'FlexibilityRating': '3', 'ProductivityRating': '1', 'SatisfactionRating': '4'}\n",
            "{'Age': '56', 'Industry': 'Healthcare', 'FlexibilityRating': '5', 'ProductivityRating': '2', 'SatisfactionRating': '3'}\n",
            "{'Age': '45', 'Industry': 'IT', 'FlexibilityRating': '4', 'ProductivityRating': '5', 'SatisfactionRating': '1'}\n",
            "{'Age': '39', 'Industry': 'Finance', 'FlexibilityRating': '3', 'ProductivityRating': '2', 'SatisfactionRating': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to collect article data within a specific year range\n",
        "def collect_articles_in_range(query, num_articles, start_year, end_year):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    collected_articles = []\n",
        "\n",
        "    while len(collected_articles) < num_articles:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"hl\": \"en\",\n",
        "            \"start\": len(collected_articles),\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, params=params)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find article entries\n",
        "        entries = soup.find_all(\"div\", class_=\"gs_ri\")\n",
        "\n",
        "        for entry in entries:\n",
        "            article_data = {}\n",
        "\n",
        "            # Year\n",
        "            year_elem = entry.find(\"div\", class_=\"gs_a\")\n",
        "            if year_elem:\n",
        "                year_text = year_elem.text.split(\" - \")[-1]\n",
        "                year = int(year_text) if year_text.isdigit() else 0\n",
        "\n",
        "                # Check if the year is within the specified range\n",
        "                if start_year <= year <= end_year:\n",
        "                    # Title\n",
        "                    title_elem = entry.find(\"h3\", class_=\"gs_rt\")\n",
        "                    article_data[\"Title\"] = title_elem.text.strip() if title_elem else \"N/A\"\n",
        "\n",
        "                    # Venue or Journal\n",
        "                    venue_elem = entry.find(\"div\", class_=\"gs_a\")\n",
        "                    article_data[\"Venue\"] = venue_elem.text.strip() if venue_elem else \"N/A\"\n",
        "\n",
        "                    # Authors\n",
        "                    authors_text = entry.find(\"div\", class_=\"gs_a\").text.split(\" - \")[0]\n",
        "                    article_data[\"Authors\"] = authors_text.strip() if authors_text else \"N/A\"\n",
        "\n",
        "                    # Abstract (if available)\n",
        "                    abstract_elem = entry.find(\"div\", class_=\"gs_rs\")\n",
        "                    article_data[\"Abstract\"] = abstract_elem.text.strip() if abstract_elem else \"N/A\"\n",
        "\n",
        "                    # Year\n",
        "                    article_data[\"Year\"] = year\n",
        "\n",
        "                    collected_articles.append(article_data)\n",
        "\n",
        "    return collected_articles\n",
        "\n",
        "# Specify the query, number of articles to collect, and the year range\n",
        "query = \"Data Science\"\n",
        "num_articles = 1000\n",
        "start_year = 2014\n",
        "end_year = 2024\n",
        "\n",
        "# Collect articles within the specified year range\n",
        "articles = collect_articles_in_range(query, num_articles, start_year, end_year)\n",
        "\n",
        "# Print the collected articles\n",
        "for i, article in enumerate(articles, start=1):\n",
        "    print(f\"Article {i}:\")\n",
        "    print(f\"Title: {article['Title']}\")\n",
        "    print(f\"Venue/Journal: {article['Venue']}\")\n",
        "    print(f\"Year: {article['Year']}\")\n",
        "    print(f\"Authors: {article['Authors']}\")\n",
        "    print(f\"Abstract: {article['Abstract']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "uQMl6g6RcYDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#I downloaded OctoPurse on my computer and saved it locally. After installing and logging in, I created a new task and entered the URL https://en.wikipedia.org/wiki/List_of_fatal_dog_attacks.  I clicked 'Save' to display the webpage and initiate data detection\n",
        "To preserve the settings, I clicked 'Create workflow' and then ran the task using 'Standard Mode' under 'Run on your device.' Once completed,\n",
        "I exported the data by clicking 'Export.' To ensure data accuracy, I chose 'Remove Duplicates' and selected the CSV format. The process was successful,\n",
        "and I confirmed the exported file contained the desired data. In just 20 steps, from downloading OctoPurse to data collection and export,\n",
        "I efficiently navigated through the web scraping process.\n",
        "https://github.com/VinaykrishnaGudla/vinaykrishna_Gudla_031/blob/main/List%20of%20fatal%20dog%20attacks%20-%20Wikipedia.csv"
      ],
      "metadata": {
        "id": "kxnXiYO0aesg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Working on web scraping tasks provided a valuable learning experience in understanding the process of extracting data from online sources. Key concepts such as HTML structure, CSS selectors, XPath, and HTTP requests were crucial in navigating and retrieving information from different websites. Understanding how to inspect web elements and identify patterns in the underlying structure of web pages proved to be essential skills in this process. Additionally, learning about different libraries and tools available for web scraping, such as Beautiful Soup and Scrapy in Python, broadened my understanding of the various approaches to data extraction.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "886fef74-b1b6-4d31-9295-4e1c6af41496"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWorking on web scraping tasks provided a valuable learning experience in understanding the process of extracting data from online sources. Key concepts such as HTML structure, CSS selectors, XPath, and HTTP requests were crucial in navigating and retrieving information from different websites. Understanding how to inspect web elements and identify patterns in the underlying structure of web pages proved to be essential skills in this process. Additionally, learning about different libraries and tools available for web scraping, such as Beautiful Soup and Scrapy in Python, broadened my understanding of the various approaches to data extraction.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t3cSfwAgabgE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "sZOhks1dXWEe"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}